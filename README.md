# ğŸ™ï¸ EchoID - Deep Voice Speaker Recognition System

![Python](https://img.shields.io/badge/python-3.8+-blue.svg)
![Keras](https://img.shields.io/badge/keras-3.11.3-red.svg)
![License](https://img.shields.io/badge/license-MIT-green.svg)

> **ğŸ“š Educational Purpose Project for Beginners**  
> EchoID is a project that started for the purpose of education for beginners who want to create a voice recognition model but couldn't or don't want to write big boilerplate codes and don't have lots of data (so we need to do augmentation which will increase the code itself). This educational purpose project features **binary classification** for speaker recognition.

> [!WARNING]
> **Educational Purpose Only:** This project is designed for learning. It is not intended for production security systems.

<div align="center">
  <br>
  <img src="https://placehold.co/800x400/EEE/31343C?text=Demo+Video+Coming+Soon&font=lato" alt="Demo Video Placeholder">
  <br>
  <em>ğŸ¥ A comprehensive walkthrough showing how to use EchoID will be available shortly.</em>
  <br>
  <br>
</div>

---

## ğŸ“‘ Table of Contents

- [About the Project](#about-the-project)
- [Features](#features)
- [How It Works](#how-it-works)
- [Installation](#installation)
- [Usage](#usage)
  - [1. Data Preparation](#1-data-preparation)
  - [2. Audio Chunking](#2-audio-chunking)
  - [3. Training the Model](#3-training-the-model)
  - [4. Real-Time Inference](#4-real-time-inference)
- [Project Structure](#project-structure)
- [Configuration](#configuration)
- [Contributing](#contributing)
- [Future Roadmap](#future-roadmap)
- [License](#license)
- [Contact](#contact)

---

## ğŸ¯ About the Project

**EchoID** is an educational deep learning project designed to help beginners learn voice speaker recognition. This project was created for those who want to build a voice recognition model without writing extensive boilerplate code or having large datasets (data augmentation is built-in to help with limited data). It leverages Convolutional Neural Networks (CNNs) trained on mel-spectrogram representations of audio signals to achieve accurate **binary classification** - distinguishing between a target speaker and others.

**This is purely for educational purposes only**

### Key Highlights:
- ğŸ§  **Deep CNN Architecture**: Custom-built CNN model optimized for speaker recognition
- ğŸµ **Mel-Spectrogram Features**: Converts raw audio to frequency-based representations
- ğŸ”Š **Advanced Augmentation**: Multi-level augmentation (waveform + spectrogram) for robust learning
- ğŸ“Š **Config-Driven**: Fully configurable via YAML for easy experimentation
- ğŸ¤ **Real-Time Inference**: GUI-based live speaker recognition
- ğŸ“ˆ **Metrics Tracking**: Comprehensive evaluation with accuracy, precision, recall, F1-score, and ROC-AUC

---

## âœ¨ Features

### Data Processing Pipeline
- **ğŸ¼ Audio Chunking**: Split long recordings into uniform 3-second segments
- **ğŸ“š Dataset Loading**: Efficient batch-wise audio loading with automatic train-test split (80/20)
- **ğŸ”„ Waveform Augmentation**: 
  - Gaussian noise injection
  - Pitch shifting (-3 to +3 semitones)
  - Amplitude scaling (0.6x to 1.2x)
- **ğŸ¶ Mel-Spectrogram Conversion**: Transform waveforms to 64x188 mel-spectrograms
- **ğŸ¨ Mel-Level Augmentation**: SpecAugment and VTLP for enhanced generalization

### Model Training
- **ğŸ—ï¸ Dynamic Model Builder**: Config-driven CNN architecture construction
- **âš¡ Smart Callbacks**: 
  - Early stopping to prevent overfitting
  - Learning rate reduction on plateau
- **ğŸ“Š Comprehensive Metrics**: Track accuracy, precision, recall, F1-score, and ROC-AUC
- **ğŸ’¾ Version Control**: Automatic model versioning and checkpointing

### Inference
- **ğŸ¯ Real-Time Recognition**: Live audio recording and prediction via GUI
- **ğŸ”‡ Voice Activity Detection (VAD)**: Energy-based VAD for robust inference
- **âš™ï¸ Configurable Thresholds**: Adjust confidence levels for predictions

---

## ğŸ” How It Works

### Complete Workflow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. DATA COLLECTION                                             â”‚
â”‚     â””â”€ Collect audio samples into data/speaker0 & data/speaker1 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  2. AUDIO CHUNKING (AudioChunker)                               â”‚
â”‚     â””â”€ Split long recordings into 3-second WAV chunks           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  3. DATASET LOADING (AudioDatasetLoader)                        â”‚
â”‚     â””â”€ Load audio files and create train/test split (80/20)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  4. WAVEFORM AUGMENTATION (AudioAugmentor)                      â”‚
â”‚     â”œâ”€ Add Gaussian noise (0.001-0.01 factor)                   â”‚
â”‚     â”œâ”€ Pitch shift (Â±3 semitones)                               â”‚
â”‚     â””â”€ Amplitude scaling (0.6x-1.2x)                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  5. MEL-SPECTROGRAM CONVERSION (WaveformToMel)                  â”‚
â”‚     â””â”€ Convert waveforms to 64x188 mel-spectrograms             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  6. MEL AUGMENTATION (MelAugmentor)                             â”‚
â”‚     â”œâ”€ SpecAugment (time & frequency masking)                   â”‚
â”‚     â””â”€ VTLP (Vocal Tract Length Perturbation)                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  7. CNN TRAINING (Trainer)                                      â”‚
â”‚     â”œâ”€ Build CNN from config (3 Conv2D layers: 32â†’64â†’128)       â”‚
â”‚     â”œâ”€ Train with Early Stopping & LR Reduction                 â”‚
â”‚     â””â”€ Track metrics: Accuracy, Precision, Recall, F1, ROC-AUC  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  8. MODEL EVALUATION                                            â”‚
â”‚     â””â”€ Evaluate on test set and generate performance reports    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  9. REAL-TIME INFERENCE (InferenceApp)                          â”‚
â”‚     â””â”€ GUI-based live speaker recognition with VAD              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Technical Details

**Input Shape**: `(64, 188, 1)` - Mel-spectrograms normalized to [0, 1]  
**CNN Architecture**: 3 convolutional blocks with progressive filters (32 â†’ 64 â†’ 128)  
**Training**: Binary cross-entropy loss with Adam optimizer  
**Sample Rate**: 16 kHz  
**Chunk Duration**: 3 seconds (48,000 samples @ 16kHz)

---

## ğŸ“¦ Installation

### Prerequisites
- Python 3.8 or higher
- pip package manager

### Install Dependencies

```bash
# Clone the repository
git clone https://github.com/Muhd-Uwais/EchoID.git
cd EchoID

# Install required packages
pip install -r requirements.txt
```

Note for Linux/Mac users: If you encounter errors installing sounddevice, you may need 
system-level dependencies:

```bash
# For Debian/Ubuntu
sudo apt-get install libasound2-dev portaudio19-dev libportaudio2 libportaudiocpp0 ffmpeg

# For MacOS
brew install portaudio ffmpeg
```

### Dependencies Overview
```
keras==3.11.3          # Deep learning framework
librosa==0.11.0        # Audio processing
numpy==2.3.5           # Numerical computing
scikit_learn==1.4.2    # Machine learning utilities
sounddevice==0.5.3     # Audio I/O
soundfile==0.13.1      # Audio file I/O
vad==1.0.2             # Voice activity detection
ruamel.base==1.0.0     # YAML configuration
```

---

## ğŸš€ Usage

### 1. Data Preparation

Organize your audio data in the following structure:

```
data/
â”œâ”€â”€ speaker0/          # Non-target speaker samples (negative class)
â”‚   â”œâ”€â”€ audio_001.wav
â”‚   â”œâ”€â”€ audio_002.wav
â”‚   â””â”€â”€ ...
â””â”€â”€ speaker1/          # Target speaker samples (positive class)
    â”œâ”€â”€ audio_001.wav
    â”œâ”€â”€ audio_002.wav
    â””â”€â”€ ...
```

**Note**: If you have long recordings, proceed to step 2 for chunking.

---

### 2. Audio Chunking

Split long audio files into 3-second chunks. The arguments define the speaker0 file counts and speaker1 file counts (e.g., 21 files for speaker0 and 23 files for speaker1):

```bash
# Usage: python chunker.py [speaker0 file count] [speaker1 file count]
python chunker.py 21 23
```

**Important**: After chunking, move all generated `.wav` files from the `chunks/` subdirectories into their respective root speaker directories (`speaker0/`, `speaker1/`), then delete the original long recordings and subdirectories.

---

### 3. Training the Model

Run the complete training pipeline using `main.py`:

```bash
python main.py
```

**What happens during training:**

1. **Load Dataset**: Audio files are loaded and split into train/test sets (80/20)
2. **Augment Training Data**: Waveform augmentation is applied (noise, pitch, amplitude)
3. **Convert to Mel-Spectrograms**: Audio is transformed into mel-spectrogram features
4. **Apply Mel Augmentation**: Additional augmentation on spectrograms
5. **Train CNN Model**: Model trains with early stopping and learning rate reduction
6. **Evaluate Performance**: Test set evaluation with comprehensive metrics

**Training Output:**
```
x_train_mel_aug shape: (40, 32, 64, 188, 1)
y_train_mel_aug shape: (40, 32)

Epoch 1/20
100/100 [==============================] - 45s 450ms/step
...
Accuracy: 0.95 | Precision: 0.94 | Recall: 0.96
```

**Trained models** are saved in: `models/cnn_model_v1/model_v1.keras`

---

### 4. Real-Time Inference

Launch the GUI application for live speaker recognition:

```bash
python inference.py
```

**GUI Features:**
- ğŸ¤ **Record Button**: Capture 3-second audio clips
- ğŸ“Š **Real-Time Prediction**: Instant speaker identification
- ğŸ¯ **Confidence Score**: Display prediction probability
- ğŸ”‡ **VAD Integration**: Filter out silence and noise

**Example Output:**
```
âœ… Target Speaker Detected (92.3%)
âŒ Other Speaker Detected (15.7%)
```

---

## ğŸ“‚ Project Structure

```
EchoID/
â”œâ”€â”€ config.yaml                    # Global configuration file
â”œâ”€â”€ chunker.py                     # Audio chunking script
â”œâ”€â”€ main.py                        # Main training script
â”œâ”€â”€ inference.py                   # Real-time inference launcher
â”œâ”€â”€ requirements.txt               # Python dependencies
â”‚
â”œâ”€â”€ data/                          # Audio dataset directory
â”‚   â”œâ”€â”€ speaker0/                  # Non-target speaker samples
â”‚   â””â”€â”€ speaker1/                  # Target speaker samples
â”‚
â”œâ”€â”€ models/                        # Trained model checkpoints
â”‚   â””â”€â”€ cnn_model_v1/             
â”‚       â””â”€â”€ model_v1.keras
â”‚
â”œâ”€â”€ notebooks/                     # Jupyter notebooks for experiments
â”‚   â”œâ”€â”€ audio_preprocessing_experimental.ipynb
â”‚   â”œâ”€â”€ model_training_experimental.ipynb
â”‚   â””â”€â”€ structure_experimental.ipynb
â”‚
â””â”€â”€ src/                           # Source code modules
    â”œâ”€â”€ data/                      # Data processing modules
    â”‚   â”œâ”€â”€ audio_chunker.py       # Split long recordings
    â”‚   â”œâ”€â”€ dataset_loader.py      # Load and batch audio
    â”‚   â”œâ”€â”€ audio_augmentor.py     # Waveform augmentation
    â”‚   â””â”€â”€ mel_processor.py       # Mel-spectrogram conversion & augmentation
    â”‚
    â”œâ”€â”€ models/                    # Model architecture & training
    â”‚   â”œâ”€â”€ model_builder.py       # Dynamic CNN builder
    â”‚   â”œâ”€â”€ trainer.py             # Training pipeline
    â”‚   â”œâ”€â”€ callbacks.py           # Keras callbacks
    â”‚   â””â”€â”€ evaluation.py          # Model evaluation metrics
    â”‚
    â”œâ”€â”€ inference/                 # Real-time inference
    â”‚   â””â”€â”€ listener.py            # GUI inference application
    â”‚
    â””â”€â”€ utils/                     # Utility functions
        â”œâ”€â”€ config_utils.py        # Config file parsing
        â””â”€â”€ metrics_utils.py       # Custom metrics
```

---

## âš™ï¸ Configuration

The entire project is configurable via `config.yaml`. Key parameters:

### Model Architecture
```yaml
model:
  input_shape: [64, 188, 1]
  filters: [32, 64, 128]
  dense_units: [512]
  dropout_rates: [0.25, 0.25, 0.3, 0.5]
  max_pool_sizes: [2, 2, 2]
  kernel_sizes: [3, 3, 3]
```

### Training Parameters
```yaml
training:
  batch_size: 32
  epochs: 20
  validation_split: 0.2
  
  early_stopping:
    enable: true
    patience: 7
    monitor: val_loss
  
  reduce_lr:
    enable: true
    factor: 0.5
    patience: 4
    min_lr: 1e-6
```

### Inference Settings
```yaml
inference:
  duration: 3              # Chunk duration (seconds)
  sample_rate: 16000       # Audio sample rate (Hz)
  threshold: 0.7           # Confidence threshold
```

---

## ğŸ¤ Contributing

Contributions are **welcome and encouraged**! Whether you're fixing bugs, improving documentation, or proposing new features, your input is valuable.

### How to Contribute

1. **Fork the repository**
2. **Create your feature branch** (`git checkout -b feature/AmazingFeature`)
3. **Commit your changes** (`git commit -m 'Add some AmazingFeature'`)
4. **Push to the branch** (`git push origin feature/AmazingFeature`)
5. **Open a Pull Request**

### Contribution Areas

We appreciate contributions in any of these areas:
- ğŸ› Bug fixes and error handling improvements
- ğŸ“ Documentation enhancements
- âœ¨ New feature implementations
- ğŸ§ª Test coverage improvements
- ğŸ¨ Code quality and refactoring

**Have ideas but unsure how to implement them?** Open an issue to discuss! We're here to help.

---

## ğŸš§ Future Roadmap

### Planned Enhancements

#### ğŸ¯ Core Features
- [ ] **Multi-Class Classification**: Extend from binary to multi-speaker recognition (3+ speakers)
- [ ] **Advanced Augmentation Methods**: 
  - [ ] Room impulse response (RIR) simulation
  - [ ] Background noise mixing
  - [ ] Speed perturbation
  - [ ] Codec simulation
- [ ] **Transfer Learning**: Leverage pre-trained models (VGGish, ResNet, wav2vec 2.0)

#### ğŸ—ï¸ Architecture Improvements
- [ ] **Better Model Architecture**: 
  - [ ] Attention mechanisms
  - [ ] Residual connections
  - [ ] Deeper networks with batch normalization
- [ ] **Data Pipeline**: 
  - [ ] TensorFlow Dataset API integration
  - [ ] Data caching for faster training

#### ğŸ“Š Model Evaluation
- [ ] **Cross-Validation**: K-fold validation for robust performance estimates
- [ ] **Confusion Matrix Visualization**: Detailed error analysis
- [ ] **Per-Speaker Performance**: Individual speaker accuracy metrics
- [ ] **Threshold Tuning**: ROC curve analysis for optimal thresholds

### Community Goals
- [ ] **Comprehensive Tutorial Series**: Step-by-step video tutorials
- [ ] **Sample Datasets**: Public dataset links and benchmarks
- [ ] **Model Zoo**: Pre-trained models for different use cases
- [ ] **Paper/Blog Post**: Detailed technical write-up of methodology

---

## ğŸ“„ License

This project is licensed under the **MIT License** - see the [LICENSE](LICENSE) file for details.

```
MIT License

Copyright (c) 2024 Muhd Uwais

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files...
```

---

## ğŸ“¬ Contact

**Muhd Uwais** - Project Author

For feedback or questions, please reach out via the link below:

- ğŸ“ **Contact Form**: [Send Me a Message](https://nox-uwi.github.io/Form/)

### Feedback & Support

- **Found a bug?** Open an [Issue](https://github.com/Muhd-Uwais/EchoID/issues)
- **Have a question?** Start a [Discussion](https://github.com/Muhd-Uwais/EchoID/discussions)
- **Want to collaborate?** Reach out via the [Contact Form](https://nox-uwi.github.io/Form/)
- **Suggestions?** We'd love to hear them!

---

## ğŸ† Star this Project!

If **EchoID** helped you with your speaker recognition tasks or you found it useful for learning, please consider giving it a â­ on GitHub! Your support motivates continued development and helps others discover the project.

---

<p align="center">
  <strong>Built with â¤ï¸ and ğŸ§  by an aspiring AI Developer</strong>
</p>

<p align="center">
  <b>#DPMG</b><br>
  <sub>Discipline â€¢ Peace â€¢ Myself â€¢ Growth</sub>
</p>

<p align="center">
  <sup>  Happy Coding! ğŸš€</sup>
</p>
